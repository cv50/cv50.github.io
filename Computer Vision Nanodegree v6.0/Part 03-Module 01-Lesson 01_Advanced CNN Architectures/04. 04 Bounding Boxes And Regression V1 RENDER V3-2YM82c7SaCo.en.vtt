WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.080
When we train a CNN to classify a set of images,

00:00:04.080 --> 00:00:06.390
we train it by comparing the output predicted

00:00:06.389 --> 00:00:09.929
class with the true class label and seeing if they match.

00:00:09.929 --> 00:00:12.629
We typically use Cross-entropy to measure the error

00:00:12.630 --> 00:00:15.480
between these classes because Cross-entropy loss

00:00:15.480 --> 00:00:18.570
decreases as the predicted class which has some uncertainty

00:00:18.570 --> 00:00:22.500
associated with it gets closer and closer to the true class label.

00:00:22.500 --> 00:00:25.350
But, when we look at comparing a set of points,

00:00:25.350 --> 00:00:30.510
say locations or points on a face or points that define a specific region in an image,

00:00:30.510 --> 00:00:34.905
we need a loss function that measures the similarity between these coordinate values.

00:00:34.905 --> 00:00:37.289
This is not a classification problem,

00:00:37.289 --> 00:00:38.939
this is a regression problem.

00:00:38.939 --> 00:00:40.710
Classification is about predicting

00:00:40.710 --> 00:00:44.484
the class label and regression is about predicting a quantity.

00:00:44.484 --> 00:00:48.479
For regression problems, like predicting X_Y coordinate locations,

00:00:48.479 --> 00:00:50.489
we need to use a loss function that compares

00:00:50.490 --> 00:00:54.164
these quantities and that gives us a measure of their closeness.

00:00:54.164 --> 00:00:57.550
It's also interesting to note that with classification problems,

00:00:57.549 --> 00:00:59.674
we have an idea of the accuracy.

00:00:59.674 --> 00:01:04.359
If our predicted class matches the true class label then our model is accurate,

00:01:04.359 --> 00:01:05.819
but with regression problems,

00:01:05.819 --> 00:01:08.414
we can't really say whether a point is accurate or not.

00:01:08.415 --> 00:01:11.040
We can only evaluate quantities by looking at

00:01:11.040 --> 00:01:13.955
something like the mean squared error between them.

00:01:13.954 --> 00:01:15.500
So for regression problems,

00:01:15.500 --> 00:01:20.385
we often talk about models with a small error rather than models that are accurate.

00:01:20.385 --> 00:01:23.320
To measure the error between two quantities,

00:01:23.319 --> 00:01:26.169
we have a few different types of loss functions that we can use.

00:01:26.170 --> 00:01:29.489
The simplest measure is L1 loss which measures

00:01:29.489 --> 00:01:33.359
the element-wise difference between a predicted output which I'll call P and a

00:01:33.359 --> 00:01:35.870
target T. Say we're predicting

00:01:35.870 --> 00:01:41.160
just one point P and X_Y coordinate that indicates the center of an object in an image,

00:01:41.159 --> 00:01:45.469
in this case, the loss function we'll look at the predicted point P that was generated by

00:01:45.469 --> 00:01:50.254
a CNN and the true target location T of the center of the object,

00:01:50.254 --> 00:01:52.549
and L1 loss would return a value that

00:01:52.549 --> 00:01:55.974
represents the distance between the predicted and true points.

00:01:55.974 --> 00:01:58.530
We also have MSE loss,

00:01:58.530 --> 00:02:00.920
which measures the mean squared error between

00:02:00.920 --> 00:02:04.019
the elements in a prediction P and a target T.

00:02:04.019 --> 00:02:07.339
Both of these methods can be good for measuring the distance between

00:02:07.340 --> 00:02:11.275
points but all loss functions have their strengths and weaknesses.

00:02:11.275 --> 00:02:15.039
You may consider that L1 loss can become negligible for

00:02:15.039 --> 00:02:20.169
small error values and the MSE loss responds the most to large errors,

00:02:20.169 --> 00:02:24.250
and so it may end up amplifying errors that are big but infrequent.

00:02:24.250 --> 00:02:26.185
Also known as outliers.

00:02:26.185 --> 00:02:31.120
There's also Smooth L1 loss which for small differences between predicted and true

00:02:31.120 --> 00:02:37.094
values uses a squared error function and for larger errors, uses L1 loss.

00:02:37.094 --> 00:02:42.770
So Smooth L1 loss try to combine the best aspects of MSE and L1 loss.

00:02:42.770 --> 00:02:45.850
It will really be up to you to try these different loss functions,

00:02:45.849 --> 00:02:47.870
look at how they decrease during training,

00:02:47.870 --> 00:02:50.800
and choose the best one for a given regression task.

