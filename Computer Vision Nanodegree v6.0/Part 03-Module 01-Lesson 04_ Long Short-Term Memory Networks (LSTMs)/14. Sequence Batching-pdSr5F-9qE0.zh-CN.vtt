WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.754
对我来说 构建神经网络最难的一步是设置正确的批次大小

00:00:04.754 --> 00:00:09.088
它更像是一个编程难题 而不是深度学习方面的问题

00:00:09.089 --> 00:00:13.964
我将逐步介绍下 RNN 的批次处理工作原理

00:00:13.964 --> 00:00:17.975
对于 RNN 我们是用序列数据训练网络

00:00:17.975 --> 00:00:20.625
例如文本 股市值 音频等

00:00:20.625 --> 00:00:24.609
通过将序列拆分为多个更短的序列

00:00:24.609 --> 00:00:28.655
我们可以利用矩阵运算 使训练更高效

00:00:28.655 --> 00:00:33.730
实际上 RNN 会同时用多个序列进行训练

00:00:33.729 --> 00:00:35.799
我们来看一个简单示例

00:00:35.799 --> 00:00:38.289
一个从 1 到 12 的数字序列

00:00:38.289 --> 00:00:41.539
我们可以将这些当做一个序列传入 RNN 中

00:00:41.539 --> 00:00:46.265
但是更好的做法是 将它一分为二并传入两个序列

00:00:46.265 --> 00:00:50.230
批次大小对应的是我们使用的序列数量

00:00:50.229 --> 00:00:52.959
因此在此示例中 批次大小是 2

00:00:52.960 --> 00:00:54.335
除了批次大小之外

00:00:54.335 --> 00:00:57.480
我们还会选择馈送到网络中的序列长度

00:00:57.479 --> 00:01:00.924
例如 假设使用长度为 3 的序列

00:01:00.924 --> 00:01:03.744
传入网络中的第一批数据是

00:01:03.744 --> 00:01:06.769
每个迷你序列中的前 3 个值

00:01:06.769 --> 00:01:09.484
下个批次包含后续 3 个值

00:01:09.484 --> 00:01:11.495
以此类推 直到数据用完了

00:01:11.495 --> 00:01:16.359
我们可以保留一个批次的隐藏状态 并用在下个批次的开头

00:01:16.359 --> 00:01:21.539
这样的话 对于每个迷你序列来说 序列信息都在批次之间传递

00:01:21.540 --> 00:01:27.200
接下来 你将了解如何构建递归网络 加油

