WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.514
在这节课

00:00:01.514 --> 00:00:04.080
你将实现一个字符级 RNN

00:00:04.080 --> 00:00:07.900
这种网络将一次一个字符地学习一段文本

00:00:07.900 --> 00:00:11.280
然后一次一个字符地生成新的文本

00:00:11.279 --> 00:00:14.574
假设我们想要生成一部新的莎士比亚剧本

00:00:14.574 --> 00:00:17.544
例如“To be or not to be.”

00:00:17.545 --> 00:00:21.710
我们逐个字符地将该序列传入 CNN 中

00:00:21.710 --> 00:00:24.780
训练过后 网络将根据已经见过的字符

00:00:24.780 --> 00:00:28.220
预测下个字符 并生成新的文本

00:00:28.219 --> 00:00:30.339
要训练此网络

00:00:30.339 --> 00:00:33.509
我们希望它能预测输入序列中的下个字符

00:00:33.509 --> 00:00:36.320
这样的话 网络将学会生成一个

00:00:36.320 --> 00:00:39.325
看起来和原始文本很像的字符序列

00:00:39.325 --> 00:00:42.705
我们来思考下这个网络的架构是怎样的

00:00:42.704 --> 00:00:44.820
首先 展开 RNN

00:00:44.820 --> 00:00:47.645
看看作为序列 整个过程的原理如何

00:00:47.645 --> 00:00:50.450
这是输入层

00:00:50.450 --> 00:00:53.240
我们以独热编码向量的形式传入字符

00:00:53.240 --> 00:00:55.630
这些向量转到隐藏层

00:00:55.630 --> 00:00:58.835
隐藏层由 LSTM 单元构成

00:00:58.835 --> 00:01:03.314
隐藏状态和单元状态从序列中的一个单元传递到下一个单元

00:01:03.314 --> 00:01:07.414
在实际操作中 我们将使用多个 LSTM 层级

00:01:07.415 --> 00:01:09.565
就像这样堆叠起来

00:01:09.564 --> 00:01:12.950
这些单元的输出转到输出层

00:01:12.950 --> 00:01:15.980
输出层用于预测下个字符

00:01:15.980 --> 00:01:18.255
我们想获得每个字符的概率

00:01:18.254 --> 00:01:21.974
就像图像分类卷积神经网络一样

00:01:21.974 --> 00:01:26.489
这意味着我们需要对输出应用 softmax 激活函数

00:01:26.489 --> 00:01:30.375
我们的目标是输入序列偏移一个单位

00:01:30.375 --> 00:01:34.219
因此每个字符都是预测序列中的下个字符

00:01:34.219 --> 00:01:38.250
同样 我们将使用交叉熵进行梯度下降训练

00:01:38.250 --> 00:01:40.170
训练好该网络后

00:01:40.170 --> 00:01:42.555
我们可以传入一个字符

00:01:42.555 --> 00:01:45.915
并获得下个潜在字符的概率分布

00:01:45.915 --> 00:01:49.750
然后 我们可以从该分布中抽样以获得下个字符

00:01:49.750 --> 00:01:52.480
接着将该字符传入网络中

00:01:52.480 --> 00:01:54.650
获得另一个字符

00:01:54.650 --> 00:01:58.800
继续下去 最终获得完整的新文本

00:01:58.799 --> 00:02:02.244
我们将使用《安娜·卡列尼娜》中的文本训练此网络

00:02:02.245 --> 00:02:04.340
这是我最喜欢的小说之一

00:02:04.340 --> 00:02:05.795
它是公开的

00:02:05.795 --> 00:02:07.605
因此你可以随意使用

00:02:07.605 --> 00:02:09.680
并且是一本很精彩的小说

