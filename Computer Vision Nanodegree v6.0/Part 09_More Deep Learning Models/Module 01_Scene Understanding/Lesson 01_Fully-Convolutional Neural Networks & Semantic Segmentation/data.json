{
  "data": {
    "lesson": {
      "id": 630772,
      "key": "51982148-54a6-4dfc-8a61-1f6c43687da8",
      "title": "Fully-Convolutional Neural Networks & Semantic Segmentation",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Get a high-level overview of how fully-convolutional neural networks work, and see how they can be used to classify every pixel in an image.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/51982148-54a6-4dfc-8a61-1f6c43687da8/630772/1544453161100/Fully-Convolutional+Neural+Networks+%26+Semantic+Segmentation+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/51982148-54a6-4dfc-8a61-1f6c43687da8/630772/1544453157725/Fully-Convolutional+Neural+Networks+%26+Semantic+Segmentation+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 341955,
          "key": "f18abbec-361e-41bb-8f19-9f087e554686",
          "title": "Intro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f18abbec-361e-41bb-8f19-9f087e554686",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350220,
              "key": "79735936-4e66-4f2a-9dcd-e1dd68edd166",
              "title": "Intro",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "1sm1EbfilXI",
                "china_cdn_id": "1sm1EbfilXI.mp4"
              }
            }
          ]
        },
        {
          "id": 341957,
          "key": "f3e2bc90-1d30-4367-84d2-35d069ab5152",
          "title": "Why Fully Convolutional Networks (FCNs) ?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f3e2bc90-1d30-4367-84d2-35d069ab5152",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350221,
              "key": "93cdc7df-03ad-4dbe-8981-a4181f774df2",
              "title": "Why Fully Convolutional Networks (FCNs) ?",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "WQ_YOz1o9GM",
                "china_cdn_id": "WQ_YOz1o9GM.mp4"
              }
            }
          ]
        },
        {
          "id": 341958,
          "key": "e5e4584d-c31a-4a1c-aa36-1ff06c608956",
          "title": "Fully Convolutional Networks",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e5e4584d-c31a-4a1c-aa36-1ff06c608956",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350223,
              "key": "35ed3eec-c1d9-412d-b3a1-0a8c31f05da6",
              "title": "Fully Convolutional Networks",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_Lh2ozg5yTs",
                "china_cdn_id": "_Lh2ozg5yTs.mp4"
              }
            }
          ]
        },
        {
          "id": 341959,
          "key": "33121278-d918-4a01-8fb8-3ad11622a83a",
          "title": "Fully Connected to 1x1 Convolution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "33121278-d918-4a01-8fb8-3ad11622a83a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350226,
              "key": "0d24b0a6-6f2d-4f5a-b1e3-5be7278989a7",
              "title": "Fully Connected to 1x1 Convolution",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "xbPtOhkJW1A",
                "china_cdn_id": "xbPtOhkJW1A.mp4"
              }
            }
          ]
        },
        {
          "id": 633857,
          "key": "d73bf8c6-0955-44b3-bd52-174a6c1dc7b0",
          "title": "Transposed Convolutions",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d73bf8c6-0955-44b3-bd52-174a6c1dc7b0",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 633860,
              "key": "fec1b547-6b01-481b-a613-8a9070e8b0bb",
              "title": "Transposed Convolutions",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "K6mlLX8ZZDs",
                "china_cdn_id": "K6mlLX8ZZDs.mp4"
              }
            },
            {
              "id": 633863,
              "key": "e99a56c5-c8f8-4ea4-8f7d-b7e00035f506",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Transposed Convolutions \nTransposed Convolutions help in [upsampling](https://en.wikipedia.org/wiki/Upsampling) the previous layer to a higher resolution or dimension.  Upsampling is a classic signal processing technique which is often [accompanied by interpolation](https://dspguru.com/dsp/faqs/multirate/interpolation/).  The term transposed can be confusing since we typically think of transposing as changing places, such as switching rows and columns of a matrix.  In this case when we use the term [transpose](http://www.dictionary.com/browse/transpose), we mean transfer to a different place or context.  \n\nWe can use a transposed convolution to transfer patches of data onto a sparse matrix, then we can fill the sparse area of the matrix based on the transferred information.  Helpful animations of convolutional operations, including transposed convolutions, can be found [here](https://github.com/vdumoulin/conv_arithmetic).  As an example, suppose you have a 3x3 input and you wish to upsample that to the desired dimension of 6x6. The process involves multiplying each pixel of your input with a kernel or filter. If this filter was of size 5x5, the output of this operation will be a weighted kernel of size 5x5. This weighted kernel then defines your output layer. \n\n## Methods of upsampling\n\n1. Max \"unpooling\"\n2. Learnable upsampling (with transposed convolutions)\n\nThe first is pictured below.",
              "instructor_notes": ""
            },
            {
              "id": 633865,
              "key": "a2718bd7-43ab-41fa-8926-8e85e96e3a19",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b1080ac_screen-shot-2018-05-31-at-4.06.11-pm/screen-shot-2018-05-31-at-4.06.11-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a2718bd7-43ab-41fa-8926-8e85e96e3a19",
              "caption": "",
              "alt": "",
              "width": 1062,
              "height": 338,
              "instructor_notes": null
            },
            {
              "id": 633864,
              "key": "3a652d1f-326c-4046-aa61-e3c2765e2777",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n## Learnable upsampling, transposed convolutions\n\nWe've been going over learnable upsampling layers. The upsampling part of the process is defined by the strides and the padding. Let's look at a simple representation of this. If we have a 2x2 input and a 3x3 kernel and a stride of 2, we can expect an output of dimension 4x4. The following image gives an idea of the process.",
              "instructor_notes": ""
            },
            {
              "id": 633866,
              "key": "7e257211-e549-4268-8712-b38345677355",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b10812c_screen-shot-2018-05-31-at-4.10.13-pm/screen-shot-2018-05-31-at-4.10.13-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7e257211-e549-4268-8712-b38345677355",
              "caption": "",
              "alt": "",
              "width": 882,
              "height": 450,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 341961,
          "key": "30527098-b8b0-419a-821b-da4473f39a72",
          "title": "Skip Connections",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "30527098-b8b0-419a-821b-da4473f39a72",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 433573,
              "key": "a6b09b91-0902-42dd-bd32-f62b03ebe2e8",
              "title": "Skip Connections",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "JUYLA5PWzo0",
                "china_cdn_id": "JUYLA5PWzo0.mp4"
              }
            }
          ]
        },
        {
          "id": 341962,
          "key": "84b6961f-535d-4bba-97d8-b8054eb56bbc",
          "title": "FCNs In The Wild",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "84b6961f-535d-4bba-97d8-b8054eb56bbc",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350229,
              "key": "166211fb-545a-40f5-8e98-78aaa99ae089",
              "title": "FCNs In The Wild",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "q9wTd53-hsw",
                "china_cdn_id": "q9wTd53-hsw.mp4"
              }
            }
          ]
        },
        {
          "id": 341951,
          "key": "234bd959-8dc5-45c2-9485-53a51455293e",
          "title": "Bounding Boxes",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "234bd959-8dc5-45c2-9485-53a51455293e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350232,
              "key": "d93742f4-6674-40d9-b054-71cdfcac748e",
              "title": "Bounding Boxes",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "uPv4d0Xl8hc",
                "china_cdn_id": "uPv4d0Xl8hc.mp4"
              }
            }
          ]
        },
        {
          "id": 341952,
          "key": "7d3f7795-204a-4706-ba6a-d3344dd3e2a2",
          "title": "Semantic Segmentation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7d3f7795-204a-4706-ba6a-d3344dd3e2a2",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350233,
              "key": "96f1134d-5b58-4d7c-934c-46a01f5f9514",
              "title": "Semantic Segmentation",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_L5gJnZrw48",
                "china_cdn_id": "_L5gJnZrw48.mp4"
              }
            }
          ]
        },
        {
          "id": 633838,
          "key": "c72e6f79-2714-435e-b3a3-326a58ce2687",
          "title": "Semantic Segmentation and FCN's",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c72e6f79-2714-435e-b3a3-326a58ce2687",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 633840,
              "key": "6f757def-fc0c-4968-a6fd-811787bee292",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Semantic Segmentation\n\nIn semantic segmentation, we want to input an image into a neural network and we want to output a category for **every pixel** in this image. For example, for the below image of a couple of cows, we want to look at every pixel and decide: is that pixel part of a cow, the grass or sky, or some other category?\n",
              "instructor_notes": ""
            },
            {
              "id": 633842,
              "key": "c1d3754d-383e-494a-953c-e4990321ab19",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b107d51_screen-shot-2018-05-31-at-3.54.39-pm/screen-shot-2018-05-31-at-3.54.39-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c1d3754d-383e-494a-953c-e4990321ab19",
              "caption": "Small image of cows in a field.",
              "alt": "",
              "width": 200,
              "height": 160,
              "instructor_notes": null
            },
            {
              "id": 633841,
              "key": "2efec0ad-894f-496b-b44d-f9a0e9ef9984",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nWe’ll have a discrete set of categories, much like in a classification task. But instead of assigning a single class to an image, we want to assign a class to every pixel in that image. So, how do we approach this task?",
              "instructor_notes": ""
            },
            {
              "id": 633843,
              "key": "5a1fafca-85b9-48d9-b0a0-7f572a226b27",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Fully-Convolutional Network (FCN) Approach\n\n\nIf your goal is to preserve the spatial information in our original input image, you might think about the simplest solution: simply never discard any of that information; never downsample/maxpool and don’t add a fully-connected layer at the end of the network.\n\nWe could use a network made entirely of convolutional layers to do this, something called a fully convolutional neural network. **A fully convolutional neural network preserves spatial information.**\n\nThis network would take in an image that has true labels attached to each pixel, so every pixel is labeled as grass or cat or sky, and so on. Then we pass that input through a stack of convolutional layers that preserve the spatial size of the input (something like 3x3 kernels with zero padding). Then, the final convolutional layer outputs a tensor that has dimensions CxHxW, where C is the number of categories we have.\n",
              "instructor_notes": ""
            },
            {
              "id": 633854,
              "key": "ff95f1ee-4d19-4f42-ae37-ee6d396c3aee",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b107f81_screen-shot-2018-05-31-at-4.04.15-pm/screen-shot-2018-05-31-at-4.04.15-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ff95f1ee-4d19-4f42-ae37-ee6d396c3aee",
              "caption": "FCN architecture.",
              "alt": "",
              "width": 500,
              "height": 320,
              "instructor_notes": null
            },
            {
              "id": 633845,
              "key": "0f367b77-9143-49e9-8a0f-fea8dba6c4f3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Predictions\nThis output Tensor of predictions contains values that classify every pixel in the image, so if we look at a single pixel in this output, we would see a vector that looks like a classification vector -- with values in it that show the probability of this single pixel being a cat or grass or sky, and so on. We could do this pixel level classification all at once, and then train this network by assigning a loss function to each pixel in the image and doing backpropagation as usual. So, if the network makes an error and classifies a single pixel incorrectly, it will go back and adjust the weights in the convolutional layers until that error is reduced.\n\n### Limitations of This Approach\n\n* It's very expensive to label this data (you have to label every pixel), and\n* It's computationally expensive to maintain spatial information in each convolutional layer\n\nSo...\n",
              "instructor_notes": ""
            },
            {
              "id": 633850,
              "key": "ff7f3866-712f-44bc-83e0-335312550dba",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Downsampling/Upsampling\n\nInstead, what you usually see is an architecture that uses downsampling of a feature map and an upsampling layer to reduce the dimensionality and, therefore, the  computational cost, of moving forward through these layers in the middle of the network.",
              "instructor_notes": ""
            },
            {
              "id": 633853,
              "key": "bf8313bf-89fe-43ad-b260-1c2eb29e699d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b107f04_screen-shot-2018-05-31-at-4.02.08-pm/screen-shot-2018-05-31-at-4.02.08-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/bf8313bf-89fe-43ad-b260-1c2eb29e699d",
              "caption": "Downsampling/upsampling an an FCN.",
              "alt": "",
              "width": 500,
              "height": 378,
              "instructor_notes": null
            },
            {
              "id": 633851,
              "key": "b2c62787-7a93-40fd-8d5d-37e7972e927d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So, what you’ll see in these networks is a couple of convolutional layers followed by downsampling done by something like maxpooling, very similar to a simple image classification network. Only, this time, in the second half of the network, we want to increase the spatial resolution, so that our output is the same size as the input image, with a label for every pixel in the original image.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 341953,
          "key": "551f2ae1-724f-42b0-8df4-30244c70cf4e",
          "title": "Scene Understanding",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "551f2ae1-724f-42b0-8df4-30244c70cf4e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350234,
              "key": "7fdb1c6e-1c65-421e-8a15-957ac52f02ea",
              "title": "Scene Understanding",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "aMQREc-mP50",
                "china_cdn_id": "aMQREc-mP50.mp4"
              }
            }
          ]
        },
        {
          "id": 341954,
          "key": "1f5dceb7-f1b1-4f06-8cae-e63c54af0ea7",
          "title": "IoU",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1f5dceb7-f1b1-4f06-8cae-e63c54af0ea7",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350235,
              "key": "0602363a-15d1-4f72-98d3-ab51492b68ce",
              "title": "IoU",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "--9BTjOsO6U",
                "china_cdn_id": "--9BTjOsO6U.mp4"
              }
            }
          ]
        },
        {
          "id": 594019,
          "key": "a3ecbb2e-afb9-4e90-a1ea-5bea5f96b595",
          "title": "IOU Example",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a3ecbb2e-afb9-4e90-a1ea-5bea5f96b595",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 594020,
              "key": "55883ab6-24cc-49da-94c9-488d0d68e030",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's walk through an example IOU calculation.\n\n## Steps\n\n+ count true positives (TP)\n+ count false positives (FP)\n+ count false negatives (FN)\n\n- Intersection = TP\n- Union = TP + FP + FN\n- IOU = Intersection/Union\n",
              "instructor_notes": ""
            },
            {
              "id": 594021,
              "key": "fa1e90e7-4106-4855-93ee-1e5ce6e192ef",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5acac2d9_legend/legend.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fa1e90e7-4106-4855-93ee-1e5ce6e192ef",
              "caption": "",
              "alt": "",
              "width": 116,
              "height": 74,
              "instructor_notes": null
            },
            {
              "id": 594024,
              "key": "8e7682d5-16de-4c5d-9605-3dc4e4551e78",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5acac3ea_example/example.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8e7682d5-16de-4c5d-9605-3dc4e4551e78",
              "caption": "",
              "alt": "",
              "width": 429,
              "height": 466,
              "instructor_notes": null
            },
            {
              "id": 594023,
              "key": "d82d6d63-4bfc-447d-9753-5830da3a6178",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the above, the left side is our ground truth, while the right side contains our predictions. The highlighted cells on the left side note which class we are looking at for statistics on the right side. The highlights on the right side note true positives in a cream color, false positives in orange, and false negatives in yellow (note that all others are true negatives - they are predicted as this individual class, and should not be based on the ground truth).\n\nWe'll look at the first class, Class 0, and you can do the same calculations from there for each.\n\nFor Class 0, only the top row of the 4x4 matrix should be predicted as zeros. This is a rather simplified version of a real ground truth - in reality, the zeros could be anywhere in the matrix. On the right side, we see 1,0,0,0, meaning the first is a false negative, but the other three are true positives (aka 3 for Intersection as well). From there, we need to find anywhere else where zero was falsely predicted, and we note that happens once on the second row, and twice on the fourth row, for a total of three false positives.\n\nTo get the Union, we add up TP (3), FP (3) and FN (1) to get seven. The IOU for this class, therefore, is 3/7.\n\nIf we do this for all the classes and average the IOUs, we get:\n\n**Mean IOU = [(3/7) + (2/6) + (3/4) + (1/6)] / 4 = 0.420**",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 630774,
          "key": "8c285fe6-a0ba-4e89-b9f6-8850c24cdd5e",
          "title": "FCN-8 Architecture",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8c285fe6-a0ba-4e89-b9f6-8850c24cdd5e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": {
            "files": [
              {
                "name": "FCN8 Paper",
                "uri": "https://video.udacity-data.com/topher/2018/May/5b10875e_fcn8-paper/fcn8-paper.pdf"
              }
            ],
            "google_plus_link": null,
            "career_resource_center_link": null,
            "coaching_appointments_link": null,
            "office_hours_link": null,
            "aws_provisioning_link": null
          },
          "atoms": [
            {
              "id": 633876,
              "key": "9969baed-0b74-46bb-8c54-b876c2bdbf18",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# FCN-8, Architecture\n\nLet’s focus on a concrete implementation of a fully convolutional network. We’ll discuss the [FCN-8 architecture](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) developed at Berkeley. In fact, many FCN models are derived from this FCN-8 implementation. The encoder for FCN-8 is the VGG16 model pretrained on ImageNet for classification. The fully-connected \"score\" layers are replaced by 1-by-1 convolutions. The code for  convolutional score layer like looks like: \n```\nself.score = nn.Conv2d(input_size, num_classes, 1)\n```\n\n The complete architecture is pictured below.",
              "instructor_notes": ""
            },
            {
              "id": 633879,
              "key": "5699b14d-84f6-49d2-9f04-ed63ddd4f156",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5b1082bd_screen-shot-2018-05-31-at-4.17.51-pm/screen-shot-2018-05-31-at-4.17.51-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5699b14d-84f6-49d2-9f04-ed63ddd4f156",
              "caption": "",
              "alt": "",
              "width": 1594,
              "height": 556,
              "instructor_notes": null
            },
            {
              "id": 633886,
              "key": "8c2c0698-37ed-467c-afbf-0b13d17c2c53",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "There are skip connections and various upsampling layers to keep track of a variety of spatial information. In practice this network is often broken down into the **encoder** portion with parameters from VGG net, and a decoder portion, which includes the upsampling layers.",
              "instructor_notes": ""
            },
            {
              "id": 633902,
              "key": "d3afb2fa-ad79-45be-a949-877731fcb03f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# FCN-8, Decoder Portion\n\nTo build the decoder portion of FCN-8, we’ll upsample the input, that comes out of the convolutional layers taken from VGG net, to the original image size. The shape of the tensor after the final convolutional transpose layer will be 4-dimensional: (batch_size, original_height, original_width, num_classes).\n\nTo define a transposed convolutional layer for upsampling, we can write the following code, where 3 is the size of the convolving kernel: \n \n```\nself.transpose = nn.ConvTranspose2d(input_depth, num_classes, 3, stride=2)\n```\n \nThe transpose convolutional layers increase the height and width dimensions of the 4D input Tensor. And you can look at the [PyTorch documentation, here](https://pytorch.org/docs/stable/nn.html#convtranspose2d).\n\n## Skip Connections\n \nThe final step is adding skip connections to the model. In order to do this we’ll combine the output of two layers. The first output is the output of the current layer. The second output is the output of a layer further back in the network, typically a pooling layer.\n\nIn the following example we combine the result of the previous layer with the result of the 4th pooling layer through elementwise addition (`tf.add`).\n \n```python\n# the shapes of these two layers must be the same to add them\ninput = input + pool_4\n```\n \nWe can then follow this with a call to our transpose layer.\n \n```python\ninput = self.transpose(input)\n```\n We can repeat this process, upsampling and creating the necssary skip connections, until the network is complete!",
              "instructor_notes": ""
            },
            {
              "id": 633904,
              "key": "82411184-47e5-4886-8ccb-180a85a34ce1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " # FCN-8, Classification & Loss\nThe final step is to define a loss. That way, we can approach training a FCN just like we would approach training a normal classification CNN. \n \nIn the case of a FCN, the goal is to assign each pixel to the appropriate class. We already happen to know a great loss function for this setup, cross entropy loss!\n\nRemember the output tensor is 4D so before we perform the loss, we have to reshape it to 2D, where each row represents a pixel and each column a class. From here we can just use standard cross entropy loss.\n \nThat’s it, we now have an idea of how to create an end-to-end model for semantic segmentation. Check out the original paper to read more specifics about FCN-8.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 341950,
          "key": "074b5110-b3dd-4b87-8504-c03c1c80d544",
          "title": "Outro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "074b5110-b3dd-4b87-8504-c03c1c80d544",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 350236,
              "key": "28f953ac-94da-4e57-9116-dadddec96772",
              "title": "Outro",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "vyNI5hdMigs",
                "china_cdn_id": "vyNI5hdMigs.mp4"
              }
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}