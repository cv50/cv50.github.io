WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.870
最终 我们希望图像说明模型接受图像作为输入

00:00:03.870 --> 00:00:08.089
并输出该图像的文本描述

00:00:08.089 --> 00:00:12.629
输入图像将由 CNN 处理 我们将 CNN 的输出与 RNN 的输入相连

00:00:12.630 --> 00:00:18.344
通过 RNN 生成描述性文本

00:00:18.344 --> 00:00:20.265
为了理解这一过程的原理

00:00:20.265 --> 00:00:23.115
我们来看一个具体的例子

00:00:23.114 --> 00:00:26.250
假设有一张训练图并且有相关的说明

00:00:26.250 --> 00:00:29.850
“A man holding a slice of pizza.”

00:00:29.850 --> 00:00:35.195
我们的目标是训练网络生成此说明 并且将此图像作为输入

00:00:35.195 --> 00:00:38.605
首先 将此图像馈送到 CNN 中

00:00:38.604 --> 00:00:43.164
我们将使用预先训练的网络 例如 VGG 16 或 ResNet

00:00:43.164 --> 00:00:44.905
在这些网络的结尾

00:00:44.905 --> 00:00:49.049
是一个输出类别得分向量的 softmax 分类器

00:00:49.049 --> 00:00:51.439
但是我们不想分类此图像

00:00:51.439 --> 00:00:56.434
我们需要的是表示该图像空间信息的一组特征

00:00:56.435 --> 00:00:58.850
为了获得这种空间信息

00:00:58.850 --> 00:01:03.380
我们将删除最后对图像分类的全连接层

00:01:03.380 --> 00:01:05.900
并查看上一个从图像中

00:01:05.900 --> 00:01:08.995
提取空间信息的层级

00:01:08.995 --> 00:01:14.189
现在 我们使用 CNN 作为特征提取器

00:01:14.189 --> 00:01:19.524
它会将原始图像中包含的大量信息压缩成更小的表示结果

00:01:19.525 --> 00:01:23.040
此 CNN 通常称为编码器

00:01:23.040 --> 00:01:27.310
因为它将图像的内容编码为更小的特征向量

00:01:27.310 --> 00:01:30.840
然后 我们可以处理此特征向量

00:01:30.840 --> 00:01:34.549
并将它当做后续 RNN 的初始输入

