WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.169
Is run the program and we find that the most likely cell

00:00:04.169 --> 00:00:08.830
is the fourth cell and it makes sense because the best match of 'red',

00:00:08.830 --> 00:00:11.564
'red' to the word is right over here, right over here.

00:00:11.564 --> 00:00:13.649
After seeing the second red,

00:00:13.650 --> 00:00:18.425
the words moved one to the right finds itself in the fourth cell as shown over here.

00:00:18.425 --> 00:00:21.300
Now, I want to celebrate with you the code that you just

00:00:21.300 --> 00:00:24.539
wrote which is a piece of software

00:00:24.539 --> 00:00:30.994
that implements the essence of Google's self-driving cars localization approach.

00:00:30.995 --> 00:00:33.064
As I said in the beginning,

00:00:33.064 --> 00:00:35.875
it's absolutely crucial that the car knows

00:00:35.875 --> 00:00:39.255
exactly where it is relative to the map of its road.

00:00:39.255 --> 00:00:42.770
Invite them, road isn't painted green and red,

00:00:42.770 --> 00:00:45.234
the road has lane markers,

00:00:45.234 --> 00:00:48.564
and instead of those green and red cells over here,

00:00:48.564 --> 00:00:53.920
we plug in the color of the lane markings relative to the color of the pavement.

00:00:53.920 --> 00:00:56.435
It's just one observation per time step,

00:00:56.435 --> 00:00:58.120
it's an entire field of observations,

00:00:58.119 --> 00:01:01.909
entire camera image, but you can do the same with the camera image.

00:01:01.909 --> 00:01:03.884
So, as long as you correspond

00:01:03.884 --> 00:01:08.019
a camera image in your model with a camera image in your measurements.

00:01:08.019 --> 00:01:10.149
Then, a piece of code,

00:01:10.150 --> 00:01:13.500
not much more difficult than what you call it yourself,

00:01:13.500 --> 00:01:17.439
is responsible for localizing the Google self-driving car.

