WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.190
一个典型的卷积神经网络可能由一系列卷积层组成

00:00:05.190 --> 00:00:10.960
随后是完全连接的图层 最终运行 softmax 激活函数

00:00:10.960 --> 00:00:14.280
对一个与分类任务相似的任务来说 这是一个非常不错的体系结构

00:00:14.279 --> 00:00:16.800
这是一个热狗的图片吗？

00:00:16.800 --> 00:00:20.469
但是如果我们想要稍微改变一下我们的任务又怎样呢？

00:00:20.469 --> 00:00:22.679
我们想回答这个问题

00:00:22.679 --> 00:00:26.189
热狗处在这张照片的什么位置？

00:00:26.190 --> 00:00:29.425
这个问题更加难以回答

00:00:29.425 --> 00:00:33.814
因为完全连接的层不保留空间信息

00:00:33.814 --> 00:00:39.534
但事实证明 如果你将 C 从连接变为卷积

00:00:39.534 --> 00:00:43.314
我们可以将卷积直接整合到图层中

00:00:43.314 --> 00:00:49.375
来创建全卷积网络 即FCN

00:00:49.375 --> 00:00:53.575
FCN会帮助我们回答这个热狗问题

00:00:53.575 --> 00:00:56.585
因为在进行卷积时它们保留

00:00:56.585 --> 00:00:59.914
整个网络的空间信息

00:00:59.914 --> 00:01:02.575
另外 由于卷积运算

00:01:02.575 --> 00:01:05.570
根本不在乎输入的大小

00:01:05.569 --> 00:01:10.689
完全卷积网络可以处理任何大小的图像

00:01:10.689 --> 00:01:15.212
在具有全连接最终层的经典卷积网络中

00:01:15.212 --> 00:01:19.834
输入的大小受全连接层大小的限制

00:01:19.834 --> 00:01:22.750
通过相同的卷积层序列传播大小不同的图像

00:01:22.750 --> 00:01:25.965
使最终输出扁平化

00:01:25.965 --> 00:01:28.314
这些输出将具有不同的大小

00:01:28.314 --> 00:01:32.064
这对于矩阵乘法来说前景不妙

00:01:32.064 --> 00:01:36.000
接下来我们将详细介绍全卷积网络

