WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.810
让我们花点时间讨论一下全卷积网络在实践中的应用

00:00:03.810 --> 00:00:05.894
一个全卷积网络有两个组件

00:00:05.894 --> 00:00:07.994
编码器和解码器

00:00:07.995 --> 00:00:12.570
我们以前提到 编码器提取了稍后将由解码器使用的功能

00:00:12.570 --> 00:00:15.780
这可能听起来为迁移学习所熟悉 实际上也是如此

00:00:15.779 --> 00:00:18.434
事实上 我们可以借用迁移学习

00:00:18.434 --> 00:00:21.504
来加速我们的全卷积网络训练

00:00:21.504 --> 00:00:26.009
在 ImageNet 上预先训练编码器是很常见的

00:00:26.010 --> 00:00:29.346
例如 VGG 和ResNet 是普遍的选择

00:00:29.346 --> 00:00:33.509
通过应用第一种逐层进行卷积层转换的特殊技术

00:00:33.509 --> 00:00:35.820
我们可以完成全卷积网络的编码器部分

00:00:35.820 --> 00:00:38.549
编码器之后是解码器

00:00:38.549 --> 00:00:41.099
解码器采用了第二种特殊技术

00:00:41.100 --> 00:00:44.710
转置卷积层来上采样图像

00:00:44.710 --> 00:00:49.120
然后通过第三种特殊技术来添加跳跃连接

00:00:49.119 --> 00:00:51.959
不过 请注意不要添加太多的跳跃连接

00:00:51.960 --> 00:00:54.980
它可能会导致模型大小的爆炸性剧增

00:00:54.979 --> 00:00:58.244
例如 当使用 VGG-16 作为编码器时

00:00:58.244 --> 00:01:02.984
通常仅将第三和第四池化层用于跳跃连接

00:01:02.984 --> 00:01:06.209
毕竟 我们现在可以连续地训练模型

