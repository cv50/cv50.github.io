WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.165
Let's see how your load takes in an input image and detects multiple objects.

00:00:05.165 --> 00:00:08.879
Say we have a CNN that's been trained to recognize several classes,

00:00:08.880 --> 00:00:10.214
including a traffic light,

00:00:10.214 --> 00:00:12.029
a car, a person, and a truck.

00:00:12.029 --> 00:00:14.419
We give it two types of anchor boxes,

00:00:14.419 --> 00:00:16.155
a tall one and a wide one,

00:00:16.155 --> 00:00:20.085
so that it can handle overlapping objects of different shapes.

00:00:20.085 --> 00:00:22.400
Once the CNN has been trained,

00:00:22.399 --> 00:00:27.129
we can now detect objects in images by feeding at new test images.

00:00:27.129 --> 00:00:29.399
The test image is first broken up into

00:00:29.399 --> 00:00:32.369
a grid and the network then produces output vectors,

00:00:32.369 --> 00:00:33.890
one for each grid cell.

00:00:33.890 --> 00:00:36.920
These vectors tell us if a cell has an object in it,

00:00:36.920 --> 00:00:38.344
what class the object is,

00:00:38.344 --> 00:00:40.615
and the bounding boxes for the object.

00:00:40.615 --> 00:00:42.660
Since we're using two anchor boxes,

00:00:42.659 --> 00:00:46.114
we'll get two predicted anchor boxes for each grid cell.

00:00:46.115 --> 00:00:51.570
Some, in fact most of the predicted anchor boxes will have a very low PC value.

00:00:51.570 --> 00:00:53.570
After producing these output vectors,

00:00:53.570 --> 00:00:58.134
we use non-maximal suppression to get rid of unlikely bounding boxes.

00:00:58.134 --> 00:01:00.739
For each class, non-maximal suppression gets rid of

00:01:00.740 --> 00:01:05.150
the bounding boxes that have a PC value lower than some given threshold.

00:01:05.150 --> 00:01:08.690
It then selects the bounding boxes with the highest PC value,

00:01:08.689 --> 00:01:11.810
and removes bounding boxes that are too similar to this.

00:01:11.810 --> 00:01:13.549
It will repeat this until all of

00:01:13.549 --> 00:01:17.420
the non-maximal bounding boxes had been removed for every class.

00:01:17.420 --> 00:01:19.590
The end result will look like this,

00:01:19.590 --> 00:01:22.010
we can see that yellow has effectively detected

00:01:22.010 --> 00:01:25.870
many objects in the image such as cars and people.

00:01:25.870 --> 00:01:28.055
Now that you know how YOLO works,

00:01:28.055 --> 00:01:32.355
you can see why it's one of the most widely used object detection algorithms today.

00:01:32.355 --> 00:01:36.829
Next, you'll get to work with the code implementation of the yellow algorithm,

00:01:36.829 --> 00:01:39.019
and really see how it detects objects in

00:01:39.019 --> 00:01:43.119
different scenes and with varying levels of confidence.

