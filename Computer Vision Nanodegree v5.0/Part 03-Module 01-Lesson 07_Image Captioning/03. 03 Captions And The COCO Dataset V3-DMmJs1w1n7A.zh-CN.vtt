WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.594
了解图像说明模型的第一步是了解模型是如何训练的

00:00:04.594 --> 00:00:08.214
模型将从一个数据集中学习规律

00:00:08.214 --> 00:00:12.254
该数据集由图像组成 并且具有描述这些图像内容的对应说明

00:00:12.255 --> 00:00:16.734
假设你需要撰写一条描述此图像的说明

00:00:16.734 --> 00:00:18.975
如何完成此任务？

00:00:18.975 --> 00:00:22.920
首先 你会查看该图像并记下各种不同的对象

00:00:22.920 --> 00:00:27.015
例如不同的人 风筝和蓝天

00:00:27.015 --> 00:00:29.550
然后根据这些对象在图像中的位置

00:00:29.550 --> 00:00:32.969
以及相互之间的关系

00:00:32.969 --> 00:00:35.679
你可能会认为 这些人在放风筝

00:00:35.679 --> 00:00:37.854
他们位于一片很大的草坪上

00:00:37.854 --> 00:00:40.289
因此可能在公园里

00:00:40.289 --> 00:00:43.100
收集这些视觉信息后

00:00:43.100 --> 00:00:47.045
你可以用一句话描述该图像 例如

00:00:47.045 --> 00:00:49.740
“People flying kites in a park.”

00:00:49.740 --> 00:00:52.950
你结合空间信息和序列文本描述

00:00:52.950 --> 00:00:56.050
撰写了一条图像说明

00:00:56.049 --> 00:00:59.849
这正是使用 CNN 和 RNN 架构的

00:00:59.850 --> 00:01:04.724
图像说明模型期望创建的流程

00:01:04.724 --> 00:01:10.679
在训练图像说明模型时常用的数据集是 COCO 数据集

00:01:10.680 --> 00:01:16.620
COCO 是 Common Objects in Context 的简称 它包含大量图像

00:01:16.620 --> 00:01:20.368
每个图像都具有大约 5 条相关图像说明

00:01:20.368 --> 00:01:24.530
这是一些说明示例

00:01:24.530 --> 00:01:29.000
接下来 你将有机会自己探索这些数据

